{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing the convNet model on real input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from src.convNet.model import convNet\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "H = W = 32\n",
    "# Define transformations\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize([H,W]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.RandomAffine(\n",
    "           # degrees=(-15,15),\n",
    "            #translate=(0,.3),\n",
    "            #scale=(.01, .2),\n",
    "            #shear=(.01, .2),\n",
    "        #),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize([H,W]),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "    ])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data_folder = \"/Users/jasperginn/PycharmProjects/Pneumonia/data/cell_images/train\"\n",
    "val_data_folder = \"/Users/jasperginn/PycharmProjects/Pneumonia/data/cell_images/val\"\n",
    "# Set up data loaders\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    root = train_data_folder,\n",
    "    transform = transform[\"train\"],\n",
    ")\n",
    "# Get classes\n",
    "#print(train_dataset.class_to_idx)\n",
    "#train_dataset.class_to_idx = {\"Uninfected\": 0, \"Parasitized\": 1}\n",
    "\n",
    "# Validation data\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root = val_data_folder,\n",
    "    transform = transform[\"test\"]\n",
    ")\n",
    "#val_dataset.class_to_idx = {\"Uninfected\": 0, \"Parasitized\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We set up the dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "#for i in range(10):\n",
    "#    batch_x, batch_y = next(iter(data_loader))\n",
    "#    print(np.shape(batch_x), batch_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "net = convNet(H, 32)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.11)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def compute_padding_size(image_dim: int, stride: int, kernel_size: int) -> int:\n",
    "    \"\"\"Compute the padding size given an input image of image_dim x image_dim,\n",
    "        a stride and a filter size\"\"\"\n",
    "    return int(math.ceil(((image_dim - 1) * stride + kernel_size - image_dim) / 2))\n",
    "\n",
    "\n",
    "def compute_layer_size_conv2d(\n",
    "    image_dim: int, stride: int, kernel_size: int, padding: int\n",
    ") -> int:\n",
    "    return int(((image_dim - kernel_size + 2 * padding) / stride) + 1)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, image_dim: int):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, padding=compute_padding_size(32, 1, 5))\n",
    "        self.pool1 = nn.MaxPool2d(3, 2, padding=compute_padding_size(32, 1, 3))\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, padding=compute_padding_size(16, 1, 5))\n",
    "        self.pool2 = nn.MaxPool2d(3, 2, padding=compute_padding_size(16, 1, 3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=compute_padding_size(8, 1, 3))\n",
    "        self.pool3 = nn.MaxPool2d(3, 2,padding=compute_padding_size(16, 1, 3))\n",
    "        self.conv4 = nn.Conv2d(64, 128, 3)\n",
    "        self.global_avg_pool = nn.AvgPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Simple convnet\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]           1,216\n",
      "         MaxPool2d-2           [-1, 16, 16, 16]               0\n",
      "            Conv2d-3           [-1, 32, 16, 16]          12,832\n",
      "         MaxPool2d-4             [-1, 32, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          18,496\n",
      "         MaxPool2d-6             [-1, 64, 4, 4]               0\n",
      "            Conv2d-7            [-1, 128, 2, 2]          73,856\n",
      "         AvgPool2d-8            [-1, 128, 1, 1]               0\n",
      "           Flatten-9                  [-1, 128]               0\n",
      "          Dropout-10                  [-1, 128]               0\n",
      "           Linear-11                   [-1, 64]           8,256\n",
      "      BatchNorm1d-12                   [-1, 64]             128\n",
      "           Linear-13                   [-1, 16]           1,040\n",
      "      BatchNorm1d-14                   [-1, 16]              32\n",
      "           Linear-15                    [-1, 1]              17\n",
      "================================================================\n",
      "Total params: 115,873\n",
      "Trainable params: 115,873\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.28\n",
      "Params size (MB): 0.44\n",
      "Estimated Total Size (MB): 0.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "net = Net(H)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "from torchsummary import summary\n",
    "summary(net, (3, H, W))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.277\n",
      "[1,    40] loss: 0.280\n",
      "[1,    60] loss: 0.320\n",
      "[1,    80] loss: 0.266\n",
      "[1,   100] loss: 0.271\n",
      "[1,   120] loss: 0.234\n",
      "[1,   140] loss: 0.284\n",
      "[1,   160] loss: 0.242\n",
      "[1,   180] loss: 0.251\n",
      "[1,   200] loss: 0.242\n",
      "[1,   220] loss: 0.232\n",
      "[1,   240] loss: 0.235\n",
      "[1,   260] loss: 0.244\n",
      "[1,   280] loss: 0.209\n",
      "[1,   300] loss: 0.237\n",
      "[1,   320] loss: 0.182\n",
      "[1,   340] loss: 0.193\n",
      "[1,   360] loss: 0.211\n",
      "[1,   380] loss: 0.196\n",
      "[1,   400] loss: 0.201\n",
      "[1,   420] loss: 0.188\n",
      "Accuracy on train set is: 0.9078352808747856\n",
      "Accuracy on validation set is: 0.8929148706896551\n",
      "[2,    20] loss: 0.205\n",
      "[2,    40] loss: 0.195\n",
      "[2,    60] loss: 0.187\n",
      "[2,    80] loss: 0.184\n",
      "[2,   100] loss: 0.203\n",
      "[2,   120] loss: 0.150\n",
      "[2,   140] loss: 0.179\n",
      "[2,   160] loss: 0.166\n",
      "[2,   180] loss: 0.184\n",
      "[2,   200] loss: 0.188\n",
      "[2,   220] loss: 0.175\n",
      "[2,   240] loss: 0.180\n",
      "[2,   260] loss: 0.186\n",
      "[2,   280] loss: 0.171\n",
      "[2,   300] loss: 0.177\n",
      "[2,   320] loss: 0.230\n",
      "[2,   340] loss: 0.220\n",
      "[2,   360] loss: 0.190\n",
      "[2,   380] loss: 0.168\n",
      "[2,   400] loss: 0.160\n",
      "[2,   420] loss: 0.186\n",
      "Accuracy on train set is: 0.9334034626929675\n",
      "Accuracy on validation set is: 0.931573275862069\n",
      "[3,    20] loss: 0.193\n",
      "[3,    40] loss: 0.187\n",
      "[3,    60] loss: 0.187\n",
      "[3,    80] loss: 0.141\n",
      "[3,   100] loss: 0.164\n",
      "[3,   120] loss: 0.154\n",
      "[3,   140] loss: 0.160\n",
      "[3,   160] loss: 0.165\n",
      "[3,   180] loss: 0.178\n",
      "[3,   200] loss: 0.154\n",
      "[3,   220] loss: 0.123\n",
      "[3,   240] loss: 0.158\n",
      "[3,   260] loss: 0.178\n",
      "[3,   280] loss: 0.161\n",
      "[3,   300] loss: 0.189\n",
      "[3,   320] loss: 0.172\n",
      "[3,   340] loss: 0.179\n",
      "[3,   360] loss: 0.121\n",
      "[3,   380] loss: 0.132\n",
      "[3,   400] loss: 0.160\n",
      "[3,   420] loss: 0.171\n",
      "Accuracy on train set is: 0.9426290469554032\n",
      "Accuracy on validation set is: 0.9147359913793103\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    acc = 0\n",
    "    batches = 0\n",
    "    for i, data in enumerate(train_data_loader, 0):\n",
    "        batch_x, batch_y = data\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass, backward pass\n",
    "        outputs = net(batch_x)\n",
    "        loss = criterion(outputs.view(-1), batch_y.type(torch.FloatTensor))\n",
    "        loss.backward()\n",
    "        # Optimize parameters\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "        batches += 1\n",
    "        outputs_class = outputs > 0\n",
    "        acc_current = torch.sum(outputs_class.view(-1) == batch_y).numpy() / batch_y.shape[0]\n",
    "        acc += acc_current\n",
    "    acc /= batches\n",
    "    print(\"Accuracy on train set is: %s\" % acc)\n",
    "    # On cross-validation set\n",
    "    with torch.no_grad():\n",
    "        acc = 0\n",
    "        batches = 0\n",
    "        for i, data in enumerate(val_data_loader, 0):\n",
    "            batch_x, batch_y = data\n",
    "            outputs = net(batch_x)\n",
    "            loss = criterion(outputs.view(-1), batch_y.type(torch.FloatTensor)).item()\n",
    "            # Predict\n",
    "            outputs_class = outputs > 0\n",
    "            acc_current = torch.sum(outputs_class.view(-1) == batch_y).numpy() / batch_y.shape[0]\n",
    "            batches += 1\n",
    "            acc += acc_current\n",
    "        acc /= batches\n",
    "        print(\"Accuracy on validation set is: %s\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set is: 0.9272629310344828\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    acc = 0\n",
    "    batches = 0\n",
    "    for i, data in enumerate(val_data_loader, 0):\n",
    "        batch_x, batch_y = data\n",
    "        outputs = net(batch_x)\n",
    "        loss = criterion(outputs.view(-1), batch_y.type(torch.FloatTensor)).item()\n",
    "        # Predict\n",
    "        outputs_class = outputs > 0\n",
    "        acc_current = torch.sum(outputs_class.view(-1) == batch_y).numpy() / batch_y.shape[0]\n",
    "        batches += 1\n",
    "        acc += acc_current\n",
    "    acc /= batches\n",
    "    print(\"Accuracy on validation set is: %s\" % acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([False,  True, False,  True,  True, False,  True,  True,  True,  True,\n         True, False,  True, False, False,  True,  True, False,  True,  True,\n         True, False, False,  True, False,  True, False,  True,  True,  True,\n        False, False,  True,  True,  True,  True, False, False,  True, False,\n         True,  True, False, False, False, False,  True,  True, False, False,\n        False,  True, False, False,  True,  True,  True, False, False,  True,\n        False,  True,  True,  True])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(outputs).view(-1).type(torch.LongTensor) == batch_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9482758620689655"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(outputs_class.view(-1) == batch_y).numpy() / batch_y.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}